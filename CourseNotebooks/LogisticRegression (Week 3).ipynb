{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - [Week 3](https://www.coursera.org/learn/machine-learning/home/week/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start off with binary classification where $0 \\leq h_\\theta(x) \\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function is a Sigmoid or Logistic function.  It looks like $h_\\theta(x)=\\frac{1}{1 + e^{-\\theta^Tx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results as a probablity of y = 1.  This looks like this mathematically:  $h_\\theta(x)=P(y=1|x;\\theta)$.  So to account for y = 1 and y = 0, the two must equal 1.  The equation looks like this: $h_\\theta(x)=P(y=0|x;\\theta)+P(y=1|x;\\theta)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function...\n",
    "$J(\\theta) = \\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}Cost(h_\\theta(x^{(i)}),y^{(i)})$ with the Cost term as...\n",
    "\n",
    "$Cost(h_\\theta(x),y) = \n",
    "    \\begin{cases}\n",
    "       -log(h_\\theta(x))     & \\quad \\text {if } y = 1\\\\\n",
    "       -log(1 - h_\\theta(x)) & \\quad \\text {if } y = 0\n",
    "    \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So the ending cost function can be written like so:\n",
    "$J(\\theta) = -\\frac{1}{m}[\\displaystyle\\sum_{i=1}^{m}y^{(i)}logh_\\theta(x^{(i)}) + (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Optimization...\n",
    "\n",
    "Can use the Octave function ```fminunc``` to compute the learning rate for you as well as tap into a highly optimized method for expediting convergence.  This works for Linear Regression as well as Logistic Regression and is useful on large ML problems.  ```fminunc``` requires a pointer to a custom cost function, an initial column vector for $\\theta$, and an ```options``` data structure. Dr. Ng calls it \"Gradient Descent on steroids\". üòÅ  See the documentation for more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fminunc' is a function from the file /usr/local/octave/3.8.0/share/octave/3.8.0/m/optimization/fminunc.m\n",
      "\n",
      " -- Function File: fminunc (FCN, X0)\n",
      " -- Function File: fminunc (FCN, X0, OPTIONS)\n",
      " -- Function File: [X, FVEC, INFO, OUTPUT, GRAD, HESS] = fminunc (FCN,\n",
      "          ...)\n",
      "     Solve an unconstrained optimization problem defined by the function\n",
      "     FCN.\n",
      "\n",
      "     FCN should accepts a vector (array) defining the unknown variables,\n",
      "     and return the objective function value, optionally with gradient.\n",
      "     In other words, this function attempts to determine a vector X such\n",
      "     that 'FCN (X)' is a local minimum.  X0 determines a starting guess.\n",
      "     The shape of X0 is preserved in all calls to FCN, but otherwise is\n",
      "     treated as a column vector.  OPTIONS is a structure specifying\n",
      "     additional options.  Currently, 'fminunc' recognizes these options:\n",
      "     \"FunValCheck\", \"OutputFcn\", \"TolX\", \"TolFun\", \"MaxIter\",\n",
      "     \"MaxFunEvals\", \"GradObj\", \"FinDiffType\", \"TypicalX\", \"AutoScaling\".\n",
      "\n",
      "     If \"GradObj\" is \"on\", it specifies that FCN, called with 2 output\n",
      "     arguments, also returns the Jacobian matrix of right-hand sides at\n",
      "     the requested point.  \"TolX\" specifies the termination tolerance in\n",
      "     the unknown variables, while \"TolFun\" is a tolerance for equations.\n",
      "     Default is '1e-7' for both \"TolX\" and \"TolFun\".\n",
      "\n",
      "     For description of the other options, see 'optimset'.\n",
      "\n",
      "     On return, FVAL contains the value of the function FCN evaluated at\n",
      "     X, and INFO may be one of the following values:\n",
      "\n",
      "     1\n",
      "          Converged to a solution point.  Relative gradient error is\n",
      "          less than specified by TolFun.\n",
      "\n",
      "     2\n",
      "          Last relative step size was less that TolX.\n",
      "\n",
      "     3\n",
      "          Last relative decrease in function value was less than TolF.\n",
      "\n",
      "     0\n",
      "          Iteration limit exceeded.\n",
      "\n",
      "     -3\n",
      "          The trust region radius became excessively small.\n",
      "\n",
      "     Optionally, fminunc can also yield a structure with convergence\n",
      "     statistics (OUTPUT), the output gradient (GRAD) and approximate\n",
      "     Hessian (HESS).\n",
      "\n",
      "     Notes: If you only have a single nonlinear equation of one variable\n",
      "     then using 'fminbnd' is usually a much better idea.  The algorithm\n",
      "     used is a gradient search which depends on the objective function\n",
      "     being differentiable.  If the function has discontinuities it may\n",
      "     be better to use a derivative-free algorithm such as 'fminsearch'.\n",
      "\n",
      "     See also: fminbnd, fminsearch, optimset.\n",
      "\n",
      "\n",
      "Additional help for built-in functions and operators is\n",
      "available in the online version of the manual.  Use the command\n",
      "'doc <topic>' to search the manual index.\n",
      "\n",
      "Help and information about Octave is also available on the WWW\n",
      "at http://www.octave.org and via the help@octave.org\n",
      "mailing list.\n"
     ]
    }
   ],
   "source": [
    "help fminunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic premise is you create a function that returns a value for $J(\\theta)$ as well as a gradient vector that contains partial derivatives given an initial column vector of \n",
    "$\\theta = \\begin{bmatrix}\n",
    "          \\theta_1 \\\\\n",
    "          \\theta_2 \\\\\n",
    "          \\vdots \\\\\n",
    "          \\theta_n \\\\\n",
    "          \\end{bmatrix}$\n",
    "          \n",
    "Each gradient index stores the partial derivative of $\\theta$ for each index of $\\theta$.  Here's an example given a cost function of $J(\\theta) = (\\theta_1 - 5)^2 + (\\theta_2 - 5)^2$ and a partial derivative for $\\theta_1$ of $\\frac{\\partial}{\\partial\\theta_1}J(\\theta) = 2(\\theta_1 - 5)$ and a partial derivative for $\\theta_2$ of $\\frac{\\partial}{\\partial\\theta_2}J(\\theta) = 2(\\theta_2 - 5)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [jVal, gradient] = costFunction(theta)\n",
    "    jVal = (theta(1)-5)^2 + (theta(2)-5)^2;\n",
    "    gradient = zeros(2,1);\n",
    "    gradient(1) = 2*(theta(1)-5);\n",
    "    gradient(2) = 2*(theta(2)-5);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optTheta =\n",
      "\n",
      "   5.0000\n",
      "   5.0000\n",
      "\n",
      "functionVal =    7.8886e-31\n",
      "exitFlag =  1\n"
     ]
    }
   ],
   "source": [
    "options = optimset('GradObj','on','MaxIter','100');\n",
    "initialTheta = zeros(2,1);\n",
    "[optTheta, functionVal, exitFlag] = fminunc(@costFunction,initialTheta,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal values of $\\theta_1$ and $\\theta_2$ respectively are 5 and 5.  The ```functionVal``` is basically 0 and the ```exitFlag``` shows that Gradient Descent did converge.  For ```fminunc``` to work, $\\theta$ needs to generally be >= a 2 dimensional column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification...\n",
    "\n",
    "Works the same as binary, but are actually training $n$ number of models given $n$ number of classes.  So, given that $y \\in \\{0,1...n\\}$ we create a classification of each class vs. every other class (the *One-vs-all* or *One-vs-rest*) strategy such that:\n",
    "\n",
    "${h_\\theta}^{(0)}(x) = P(y = 0|x;\\theta)$...${h_\\theta}^{(n)}(x) = P(y = n|x;\\theta)$\n",
    "\n",
    "You then would make your prediction based on the maximum probability revealed by each classification model such that:\n",
    "\n",
    "$prediction = max_i(h_\\theta^{(i)}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting...\n",
    "\n",
    "Underfitting = *High Bias* meaning that a model has a preconceived notion of how the data should fit despite the data's \"evidence to the contrary\" per Dr. Ng.  Doesn't even fit the training data very well.\n",
    "\n",
    "Overfitting = *High Variance* meaning the \"space of hypotheses is too large or too variable and we don't have enough data to constrain it to give us a good hypothesis\" - Dr. Ng.  Doesn't generalize very well but fits the training data *too* well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
